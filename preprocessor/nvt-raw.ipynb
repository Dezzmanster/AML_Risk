{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 20 13:39:37 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    54W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:1E:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    52W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import nvidia_smi\n",
    "\n",
    "# External Dependencies\n",
    "import cupy as cp\n",
    "import cudf\n",
    "import dask_cudf\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask.utils import parse_bytes\n",
    "from dask.delayed import delayed\n",
    "import dask.dataframe as dd\n",
    "import rmm\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NVTabular\n",
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops\n",
    "from nvtabular.io import Shuffle\n",
    "from nvtabular.utils import device_mem_size\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Version: 2.30.0\n",
      "Dask cuDF Version: 0.16.0\n",
      "\n",
      "\n",
      " n_gpus_avail: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:43809</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>404.32 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:43809' processes=2 threads=2, memory=404.32 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dashboard avail: http://localhost:8888/proxy/8787/status\n"
     ]
    }
   ],
   "source": [
    "from fencoding_GPUs import set_cluster_client\n",
    "client = set_cluster_client(n_gpus=-1, device_spill_frac=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home-credit-default-risk tables\n",
    "if not Path(\"data/application_test.csv\").is_file():\n",
    "    %cd data\n",
    "    ! wget https://www.dropbox.com/s/j9xwcj9ixki5t2l/home-credit-default-risk.zip?dl=0 -O data.zip\n",
    "    ! unzip -q data.zip\n",
    "    ! rm data.zip\n",
    "# default-of-credit-card-clients-dataset\n",
    "if not Path(\"data/default_ucr.csv\").is_file():\n",
    "    %cd data\n",
    "    ! wget https://www.dropbox.com/s/lj0d7qez18ea7dx/UCI_Credit_Card.csv?dl=0 -O default_ucr.csv\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the source datasets\n",
    "dict_ = {\n",
    "    'datasets':[\n",
    "                # default-of-credit-card-clients-datasets\n",
    "                pd.read_csv('./data/default_ucr.csv'),\n",
    "    ],\n",
    "\n",
    "    'name_dropped_columns':\n",
    "                [\n",
    "                 # default-of-credit-card-clients-datasets\n",
    "                 ['ID', 'default.payment.next.month']\n",
    "                 #['default.payment.next.month'] # 'ID' is needed for shuffling\n",
    "    ],   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_D = 0\n",
    "X= dict_['datasets'][N_D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FEncoding_advanced(object):   \n",
    "    def __init__(self, client, rest_col_names=[], y_names=[], filename=None):        \n",
    "        self.filename = filename\n",
    "        self.rest_col_names = rest_col_names\n",
    "        self.y_names = y_names\n",
    "        self.n_gpus = len(client.nthreads())\n",
    "        self.client = client\n",
    "        self.output_path=\"./parquet_data_tmp\"\n",
    "        self.categor_types = ['category', 'object', 'bool', 'int32', 'int64', 'int8']\n",
    "        self.numer_types = ['float', 'float32', 'float64']\n",
    "        self.time_types = ['datetime64[ns]', 'datetime64[ns, tz]'] \n",
    "        # What else? https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
    "        # TODO: check if there are any other time types\n",
    "        \n",
    "    def elim_empty_columns(self, X, save_to_csv = False):\n",
    "        # GPU version\n",
    "        ddf = dd.from_pandas(X, npartitions=self.n_gpus)\n",
    "        cols_to_drop = []\n",
    "        for column in ddf.columns:\n",
    "            if len(ddf[column].unique().compute().values) < 2:\n",
    "                cols_to_drop.append(column)\n",
    "        print('\\n dropped columns:', cols_to_drop)\n",
    "        X_final = ddf.drop(cols_to_drop, axis=1).compute()  \n",
    "        if filename is not None:\n",
    "            if save_to_csv:\n",
    "                X_final.to_csv('./data/' + self.filename, index=False)\n",
    "        return X_final\n",
    "    \n",
    "    def initialize_types(self, X, return_dtype=False, save_to_pkl = False, dict_name = 'out_dict.pkl'):\n",
    "        # GPU version\n",
    "        X = dd.from_pandas(X, npartitions=2)       \n",
    "        self.categor_columns, self.numer_columns, self.time_columns = [], [], []\n",
    "        # Sometimes categorical feature can be presented with a float type. Let's check for that\n",
    "        f_columns_names =[x for x in list(X.columns)  if x not in self.rest_col_names + self.y_names]\n",
    "        for column in f_columns_names:\n",
    "            c_type = str(X[column].dtype) \n",
    "            if any(c_type == t for t in self.numer_types):    \n",
    "                uvs = cp.array(X[column].unique().compute())\n",
    "                unique_values = list(uvs[~cp.isnan(uvs)])\n",
    "                if cp.array([el.item().is_integer() for el in unique_values]).sum() == len(unique_values):\n",
    "                    #print('\\n {} has type {} and number of unique values: {}, will be considered as a categorical \\n'.format(column, c_type, len(unique_values)))\n",
    "                    #logging.info(f\"{column} has type {c_type} and number of unique values: {len(unique_values)}, will be considered as a categorical\")\n",
    "                    self.categor_columns.append(column)\n",
    "                else:\n",
    "                    self.numer_columns.append(column)\n",
    "            if any(c_type == t for t in self.categor_types):\n",
    "                self.categor_columns.append(column)\n",
    "            if any(c_type == t for t in self.time_types):\n",
    "                self.time_columns.append(column)                             \n",
    "        out_dict =  {'categor_columns': self.categor_columns,\n",
    "                'numer_columns': self.numer_columns,\n",
    "                'time_columns': self.time_columns,                    \n",
    "         }\n",
    "        if return_dtype:\n",
    "            out_dict.update(\n",
    "                {'categor_columns_dtypes': [str(X[self.categor_columns].dtypes.values[i]) for i in range(len(self.categor_columns))],\n",
    "                 'numer_columns_dtypes': [str(X[self.numer_columns].dtypes.values[i]) for i in range(len(self.numer_columns))],\n",
    "                 'time_columns_dtypes': [str(X[self.time_columns].dtypes.values[i]) for i in range(len(self.time_columns))],                    \n",
    "             })            \n",
    "        if save_to_pkl:\n",
    "            output = open('./data/' + dict_name, 'wb')\n",
    "            pickle.dump(out_dict, output)\n",
    "            output.close()\n",
    "        return out_dict\n",
    "    \n",
    "    def date_replace(self, X, save_to_csv = False):\n",
    "        # GPU version\n",
    "        return X\n",
    "        \n",
    "    \n",
    "    def processing(self, X, \n",
    "                  outliers_detection_technique = 'iqr_proximity_rule', #'gaussian_approximation','quantiles'\n",
    "                  fill_with_value = 'zeros', #'extreme_values', #'mean-median'\n",
    "                  encoding_method = 'OrdinalEncoder', #'OneHotEncoder'\n",
    "                  save_to_csv = False,\n",
    "                  ):\n",
    "        \n",
    "        self.initialize_types(X,  return_dtype=False)\n",
    "        dataset = nvt.Dataset(X)\n",
    "        \n",
    "        # Initalize our Workflow\n",
    "        workflow = nvt.Workflow(cat_names=self.categor_columns, \n",
    "                        cont_names=self.numer_columns,\n",
    "                        label_name=self.y_names,\n",
    "                        client=self.client\n",
    "                       )\n",
    "        \n",
    "        # Operators: https://nvidia.github.io/NVTabular/main/api/ops/index.html\n",
    "        # OutlDetect\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        workflow.add_preprocess(\n",
    "            #TODO: change in OutlDetect \n",
    "            ops.Clip(0, 10, columns=)#min_value=None, max_value=None, columns=f_dict['numer_columns'], replace=True)\n",
    "\n",
    "            #TODO: change in encode_categor\n",
    "            #ops.TargetEncoding(cat_groups=f_dict['categor_columns'],\n",
    "             #                  cont_target=None),\n",
    "\n",
    "            #TODO: chenge in tree-based models, nana will be filled in with max values (or zeros)\n",
    "            #ops.FillMissing(fill_val=0, columns=f_dict['categor_columns'] + f_dict['numer_columns'], replace=True),\n",
    "        )\n",
    "\n",
    "        #workflow.add_preprocess(\n",
    "        #    ops.Categorify(10)\n",
    "        #)\n",
    "\n",
    "        #workflow.add_preprocess(\n",
    "        #    ops.FillMedian()#columns=f_dict['categor_columns'], preprocessing=True, replace=True)\n",
    "\n",
    "        #)     \n",
    "        \n",
    "        #######################################################        \n",
    "        workflow.finalize()\n",
    "        tmp_output_path=\"./parquet_data_tmp\"\n",
    "        workflow.apply(\n",
    "            dataset,\n",
    "             output_format=\"parquet\",\n",
    "             output_path=tmp_output_path,\n",
    "             shuffle=Shuffle.PER_WORKER,  # Shuffle algorithm\n",
    "             out_files_per_proc=8, # Number of output files per worker\n",
    "        )\n",
    "        files = glob.glob(tmp_output_path + \"/*.parquet\")\n",
    "        X_final = cudf.read_parquet(files[0])\n",
    "        for i in range(1, len(files)):    \n",
    "            X_final = X_final.append(cudf.read_parquet(files[i]))      \n",
    "        \n",
    "        # Delete temporary files\n",
    "        shutil.rmtree(tmp_output_path, ignore_errors=True)\n",
    "        shutil.rmtree('dask-worker-space', ignore_errors=True)\n",
    "        \n",
    "        if save_to_csv is not None: \n",
    "            X_final.to_csv('./data/' + self.filename, index=False)\n",
    "        return X_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fencoding = FEncoding_advanced(client)\n",
    "fencoding.rest_col_names = ['ID']\n",
    "fencoding.y_names = ['default.payment.next.month']\n",
    "\n",
    "# Ready, TODO: check for correctness \n",
    "#fencoding.elim_empty_columns(X)\n",
    "#fencoding.initialize_types(X, return_dtype=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-01cc5476175c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b3bcd26214d0>\u001b[0m in \u001b[0;36mprocessing\u001b[0;34m(self, X, outliers_detection_technique, fill_with_value, encoding_method, save_to_csv)\u001b[0m\n\u001b[1;32m     94\u001b[0m         workflow.add_preprocess(\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m#TODO: change in OutlDetect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categor_columns'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#min_value=None, max_value=None, columns=f_dict['numer_columns'], replace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m#TODO: change in encode_categor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_dict' is not defined"
     ]
    }
   ],
   "source": [
    "fencoding.processing(X, save_to_csv = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
