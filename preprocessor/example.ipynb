{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yILP-zCWp_bG"
      },
      "source": [
        "import os, copy, time\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# We will use multiprocessing for simple additive processes. Further, we will expend for other cases.\n",
        "# Sources: \n",
        "#https://medium.com/@vasista/parallel-processing-with-pandas-c76f88963005\n",
        "#https://towardsdatascience.com/speeding-up-and-perfecting-your-work-using-parallel-computing-8bc2f0c073f8\n",
        "import multiprocessing as mp\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# home-credit-default-risk tables\n",
        "if not Path(\"/content/application_test.csv\").is_file():\n",
        "  ! wget https://www.dropbox.com/s/j9xwcj9ixki5t2l/home-credit-default-risk.zip?dl=0 -O data.zip\n",
        "  ! unzip -q data.zip\n",
        "\n",
        "# default-of-credit-card-clients-dataset\n",
        "if not Path(\"/content/default_ucr.csv\").is_file():\n",
        "  ! wget https://www.dropbox.com/s/lj0d7qez18ea7dx/UCI_Credit_Card.csv?dl=0 -O default_ucr.csv\n",
        "\n",
        "from preprocessing import (FCleaning, \n",
        "                           FEncoding,\n",
        ")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7j2FtHo2WhL",
        "outputId": "054e8322-737e-49de-a6ba-04cc79bd33c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Read in the source datasets\n",
        "dict = {\n",
        "    'datasets':[\n",
        "                # home-credit-default-risk tables\n",
        "                pd.read_csv('/content/application_train.csv'),\n",
        "                pd.read_csv('/content/application_test.csv'),\n",
        "                pd.read_csv('/content/bureau.csv'),\n",
        "                pd.read_csv('/content/bureau_balance.csv'),\n",
        "                pd.read_csv('/content/POS_CASH_balance.csv'),\n",
        "                pd.read_csv('/content/credit_card_balance.csv'),\n",
        "                # pd.read_csv('/content/previous_application.csv'),\n",
        "                # pd.read_csv('/content/installments_payments.csv'),\n",
        "\n",
        "                # default-of-credit-card-clients-datasets\n",
        "                # pd.read_csv('/content/default_ucr.csv'),\n",
        "    ],\n",
        "\n",
        "    'name_dropped_columns':\n",
        "                [\n",
        "                 # home-credit-default-risk tables\n",
        "                 ['SK_ID_CURR', 'TARGET'],\n",
        "                 ['SK_ID_CURR'],\n",
        "                 ['SK_ID_CURR',\t'SK_ID_BUREAU'],\n",
        "                 ['SK_ID_BUREAU'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "\n",
        "                 # default-of-credit-card-clients-datasets\n",
        "                 ['ID', 'default.payment.next.month']              \n",
        "    ],   \n",
        "}\n",
        "\n",
        "# Keep ID and target columns separately\n",
        "dict['dropped_columns'] = [dict['datasets'][i][dict['name_dropped_columns'][i]] for i in range(len(dict['datasets']))]\n",
        "\n",
        "# Drop ID and target columns from the tables\n",
        "dict['datasets'] = [dict['datasets'][i].drop(dict['name_dropped_columns'][i], axis=1) for i in range(len(dict['datasets']))]\n",
        "\n",
        "[np.unique([str(dict['datasets'][i][column].dtype) for column in dict['datasets'][i].columns]) for i in range(len(dict['datasets']))]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['int64', 'object'], dtype='<U6'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lEyN2ye85kr",
        "outputId": "251d32d9-30bc-4c59-c8f8-b8bfb14aaa1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "X = dict['datasets'][0]\n",
        "\n",
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 1,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "\n",
        "s_time = time.time()\n",
        "fcleaning.emptyness_elimination(X)\n",
        "fcleaning.outliers_elimination(X)\n",
        "print('\\n n_jobs = 1, time:', time.time() - s_time)\n",
        "\n",
        "print('\\n CPUs avail:{} \\n'.format(mp.cpu_count()))\n",
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 2,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "\n",
        "s_time = time.time()\n",
        "fcleaning.emptyness_elimination(X)\n",
        "fcleaning.outliers_elimination(X)\n",
        "print('\\n n_jobs = 2, time:', time.time() - s_time)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " n_jobs = 1, time: 6.739346981048584\n",
            "\n",
            " CPUs avail:2 \n",
            "\n",
            "\n",
            " n_jobs = 2, time: 6.865135669708252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhMZDKFC28lW",
        "outputId": "f2a9b290-afa9-4842-c23b-63828e485162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 2,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "\n",
        "for i in range(len(dict['datasets'])):\n",
        "    X = dict['datasets'][i]\n",
        "    print('\\n dataset N {} | size: {} | emptyness_elimination ...'.format(i, X.shape))\n",
        "    s_time = time.time()\n",
        "    fcleaning.emptyness_elimination(X)   \n",
        "    print('\\n dataset N {}, emptyness_elimination completed successfully, time: {}'.format(i, time.time() - s_time))\n",
        "\n",
        "    s_time = time.time()\n",
        "    print('\\n dataset N {}  | size: {} | outliers_elimination ...'.format(i, X.shape))\n",
        "    fcleaning.outliers_elimination(X)\n",
        "    print('\\n dataset N {}, outliers_elimination completed successfully, time: {}'.format(i, time.time() - s_time))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " dataset N 0 | size: (307511, 120) | emptyness_elimination ...\n",
            "\n",
            " dataset N 0, emptyness_elimination completed successfully, time: 2.7245032787323\n",
            "\n",
            " dataset N 0  | size: (307511, 120) | outliers_elimination ...\n",
            "\n",
            " dataset N 0, outliers_elimination completed successfully, time: 4.1842169761657715\n",
            "\n",
            " dataset N 1 | size: (48744, 120) | emptyness_elimination ...\n",
            "\n",
            " FLAG_DOCUMENT_2, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_10, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_12, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_13, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_14, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_15, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_16, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_17, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_19, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_20, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_21, unique values: [0]\n",
            "\n",
            " dataset N 1, emptyness_elimination completed successfully, time: 0.6694204807281494\n",
            "\n",
            " dataset N 1  | size: (48744, 120) | outliers_elimination ...\n",
            "\n",
            " dataset N 1, outliers_elimination completed successfully, time: 0.9467957019805908\n",
            "\n",
            " dataset N 2 | size: (1716428, 15) | emptyness_elimination ...\n",
            "\n",
            " dataset N 2, emptyness_elimination completed successfully, time: 2.671640634536743\n",
            "\n",
            " dataset N 2  | size: (1716428, 15) | outliers_elimination ...\n",
            "\n",
            " dataset N 2, outliers_elimination completed successfully, time: 3.4559085369110107\n",
            "\n",
            " dataset N 3 | size: (27299925, 2) | emptyness_elimination ...\n",
            "\n",
            " dataset N 3, emptyness_elimination completed successfully, time: 6.400071144104004\n",
            "\n",
            " dataset N 3  | size: (27299925, 2) | outliers_elimination ...\n",
            "\n",
            " dataset N 3, outliers_elimination completed successfully, time: 7.082038164138794\n",
            "\n",
            " dataset N 4 | size: (10001358, 6) | emptyness_elimination ...\n",
            "\n",
            " dataset N 4, emptyness_elimination completed successfully, time: 7.526120901107788\n",
            "\n",
            " dataset N 4  | size: (10001358, 6) | outliers_elimination ...\n",
            "\n",
            " dataset N 4, outliers_elimination completed successfully, time: 9.860339164733887\n",
            "\n",
            " dataset N 5 | size: (3840312, 21) | emptyness_elimination ...\n",
            "\n",
            " dataset N 5, emptyness_elimination completed successfully, time: 8.498157262802124\n",
            "\n",
            " dataset N 5  | size: (3840312, 21) | outliers_elimination ...\n",
            "\n",
            " dataset N 5, outliers_elimination completed successfully, time: 10.86488676071167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L71oKQlpKba7",
        "outputId": "bf6a1321-9f82-4f4b-8e0f-04802d1c1406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X = dict['datasets'][2]\n",
        "\n",
        "fencoding = FEncoding(n_jobs = 1, \n",
        "                      chunks = None, \n",
        "                      path = None,)\n",
        "s_time = time.time()\n",
        "fencoding.dtime_to_data(X, \n",
        "                        dtime_col_names = {\n",
        "                                              'ddays' : ['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE'],\n",
        "                                              'dmonths' : [],\n",
        "                                              'dyears' : []\n",
        "                                           },               \n",
        "                        time_encode = True, \n",
        "                        drop_current = True\n",
        "                     )\n",
        "f_dict = fencoding.pick_categor(X)\n",
        "print('\\n f_dict:', f_dict)\n",
        "fencoding.bucket_numerical(X, \n",
        "                         n_bins=5, \n",
        "                         columns_to_buck = 'all_numerical', \n",
        "                         drop_current = True)\n",
        "\n",
        "fencoding.encode_categor(X, method = 'OrdinalEncoder')\n",
        "print('\\n n_jobs = 1, time:', time.time() - s_time)\n",
        "\n",
        "fencoding = FEncoding(n_jobs = 2, \n",
        "                      chunks = None, \n",
        "                      path = None,)\n",
        "s_time = time.time()\n",
        "fencoding.dtime_to_data(X, \n",
        "                        dtime_col_names = {\n",
        "                                              'ddays' : ['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE'],\n",
        "                                              'dmonths' : [],\n",
        "                                              'dyears' : []\n",
        "                                           },               \n",
        "                        time_encode = True, \n",
        "                        drop_current = True\n",
        "                     )\n",
        "f_dict = fencoding.pick_categor(X)\n",
        "print('\\n f_dict:', f_dict)\n",
        "fencoding.bucket_numerical(X, \n",
        "                         n_bins=5, \n",
        "                         columns_to_buck = 'all_numerical', \n",
        "                         drop_current = True)\n",
        "\n",
        "fencoding.encode_categor(X, method = 'OrdinalEncoder')\n",
        "print('\\n n_jobs = 1, time:', time.time() - s_time)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " DAYS_CREDIT processed ...\n",
            "\n",
            " DAYS_CREDIT_ENDDATE processed ...\n",
            "\n",
            " DAYS_ENDDATE_FACT processed ...\n",
            "\n",
            " DAYS_CREDIT_UPDATE processed ...\n",
            "\n",
            " DAYS_CREDIT_date was encoded\n",
            "\n",
            " DAYS_CREDIT_ENDDATE_date was encoded\n",
            "\n",
            " DAYS_ENDDATE_FACT_date was encoded\n",
            "\n",
            " DAYS_CREDIT_UPDATE_date was encoded\n",
            "\n",
            " f_dict: {'categor_columns': ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'CNT_CREDIT_PROLONG', 'CREDIT_TYPE', 'DAYS_CREDIT_UPDATE'], 'numer_columns': ['DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_ANNUITY'], 'time_columns': []}\n",
            "\n",
            " DAYS_CREDIT_ENDDATE bucketing ...\n",
            "\n",
            " DAYS_ENDDATE_FACT bucketing ...\n",
            "\n",
            " AMT_CREDIT_MAX_OVERDUE bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_DEBT bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_LIMIT bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_OVERDUE bucketing ...\n",
            "\n",
            " AMT_ANNUITY bucketing ...\n",
            "\n",
            " n_jobs = 1, time: 19.419448852539062\n",
            "\n",
            " DAYS_CREDIT processed ...\n",
            "\n",
            " DAYS_CREDIT_UPDATE processed ...\n",
            "\n",
            " DAYS_CREDIT_ENDDATE processed ...\n",
            "\n",
            " DAYS_ENDDATE_FACT processed ...\n",
            "\n",
            " DAYS_CREDIT_UPDATE_date was encoded\n",
            "\n",
            " DAYS_CREDIT_date was encoded\n",
            "\n",
            " DAYS_CREDIT_ENDDATE_date was encoded\n",
            "\n",
            " DAYS_ENDDATE_FACT_date was encoded\n",
            "\n",
            " f_dict: {'categor_columns': ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'CNT_CREDIT_PROLONG', 'CREDIT_TYPE', 'DAYS_CREDIT_UPDATE'], 'numer_columns': ['DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_ANNUITY'], 'time_columns': []}\n",
            "\n",
            " AMT_CREDIT_SUM bucketing ...\n",
            "\n",
            " DAYS_CREDIT_ENDDATE bucketing ...\n",
            "\n",
            " DAYS_ENDDATE_FACT bucketing ...\n",
            " AMT_CREDIT_SUM_DEBT bucketing ...\n",
            "\n",
            "\n",
            " AMT_CREDIT_SUM_LIMIT bucketing ...\n",
            "\n",
            " AMT_CREDIT_MAX_OVERDUE bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_OVERDUE bucketing ...\n",
            "\n",
            " AMT_ANNUITY bucketing ...\n",
            "\n",
            " n_jobs = 1, time: 15.008081436157227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6FGGFT2H1eJ"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-IIdbfRIGEi"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys5_F0KZH1nB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0k_4uy6H1g7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djhAiqWC9bNQ"
      },
      "source": [
        "class FImputation(object):\n",
        "    '''\n",
        "      Dealing with missing values:\n",
        "      We will use simple techniques with regards to the model that we use.\n",
        "      For tree-based models, nana will be filled in with max values (or zeros)\n",
        "      For regression with means and medians for numerical and categorical types respectively.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, model_type, fill_with_value = None, \n",
        "                    n_jobs = 1, chunks = None, \n",
        "                    path = None,\n",
        "                    ):\n",
        "        \n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.fill_with_value = fill_with_value\n",
        "        \n",
        "        self.n_jobs = n_jobs\n",
        "        self.chunks = chunks\n",
        "        self.path = path\n",
        "\n",
        "\n",
        "    def impute_(self, X):\n",
        "        if self.model_type == 'tree-based':                         \n",
        "            imputer = SimpleImputer(missing_values=[np.nan], # what else?\n",
        "                                    strategy='constant'\n",
        "                                    )\n",
        "            if self.fill_with_value == 'zeros':\n",
        "                #imputer.set_params(fill_value = 0)\n",
        "                #return imputer.fit_transform(X)\n",
        "                X.fillna(0, inplace=True)                        \n",
        "                return X\n",
        "\n",
        "            elif self.fill_with_value == 'extreme_values':\n",
        "                for column in X.columns:\n",
        "                    #imputer.set_params(fill_value = X['AMT_INCOME_TOTAL'][abs(X['AMT_INCOME_TOTAL']) == abs(X['AMT_INCOME_TOTAL']).max()].item())\n",
        "                    #X[column] = imputer.fit_transform(np.array(X[column].values).reshape(-1,1))\n",
        "                    X[column].fillna( X['AMT_INCOME_TOTAL'][abs(X['AMT_INCOME_TOTAL']) == abs(X['AMT_INCOME_TOTAL']).max()].item(), inplace=True)\n",
        "                return X\n",
        "            else:\n",
        "                raise VlaueError('Identify fill_with_value parameter')\n",
        "\n",
        "        if self.model_type == 'regression-based':\n",
        "            #TODO\n",
        "            strategies = ['mean', 'median']\n",
        "            col_types = []\n",
        "\n",
        "    def impute(self, X):\n",
        "        if self.chunks == None:\n",
        "            self.chunks  = int(len(X.columns)/self.n_jobs)\n",
        "            p = Pool(processes = self.n_jobs)\n",
        "            X =  pd.concat(p.map(self.impute_, \n",
        "                                    [X[list(X.columns)[start: start + self.chunks]] for start in range(0, len(X.columns), self.chunks)]\n",
        "                                    ), axis=1)\n",
        "            if self.path != None:\n",
        "                  X.to_csv(self.path)\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Guv6qbCUz4r",
        "outputId": "15b0b3d9-f36e-4996-b664-b91d9ae0941b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fimputation = FImputation('tree-based',\n",
        "                          fill_with_value='zeros',\n",
        "                          n_jobs=1\n",
        "                         )\n",
        "s_time = time.time()\n",
        "fimputation.impute(X)\n",
        "time.time() - s_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.768680095672607"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM1vnnTp-oxs",
        "outputId": "228f4717-5106-4c8c-d76b-e2a513fa2640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fimputation = FImputation('tree-based',\n",
        "                          fill_with_value='zeros',\n",
        "                          n_jobs=2\n",
        "                         )\n",
        "s_time = time.time()\n",
        "fimputation.impute(X)\n",
        "time.time() - s_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.519124746322632"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqml3lx3Un3h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R76KT0ZebVWN"
      },
      "source": [
        "class FScaling(object):\n",
        "  def __init__(self, scaler):\n",
        "    self.scaler = scaler\n",
        "\n",
        "  def scale(X):\n",
        "\n",
        "    return X\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}