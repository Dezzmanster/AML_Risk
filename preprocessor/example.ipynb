{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yILP-zCWp_bG"
      },
      "source": [
        "import os, copy, time\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# We will use multiprocessing for simple additive processes. Further, we will expend for other cases.\n",
        "# Sources: \n",
        "#https://medium.com/@vasista/parallel-processing-with-pandas-c76f88963005\n",
        "#https://towardsdatascience.com/speeding-up-and-perfecting-your-work-using-parallel-computing-8bc2f0c073f8\n",
        "import multiprocessing as mp\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# home-credit-default-risk tables\n",
        "if not Path(\"./application_test.csv\").is_file():\n",
        "  ! wget https://www.dropbox.com/s/j9xwcj9ixki5t2l/home-credit-default-risk.zip?dl=0 -O data.zip\n",
        "  ! unzip -q data.zip\n",
        "\n",
        "# default-of-credit-card-clients-dataset\n",
        "if not Path(\"./default_ucr.csv\").is_file():\n",
        "  ! wget https://www.dropbox.com/s/lj0d7qez18ea7dx/UCI_Credit_Card.csv?dl=0 -O default_ucr.csv\n",
        "\n",
        "from preprocessing import (FCleaning, \n",
        "                           FEncoding,\n",
        "                           FImputation\n",
        ")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7j2FtHo2WhL",
        "outputId": "8e69e624-f670-4664-8245-9f2929db10cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Read in the source datasets\n",
        "dict = {\n",
        "    'datasets':[\n",
        "                # home-credit-default-risk tables\n",
        "                pd.read_csv('/content/application_train.csv'),\n",
        "                pd.read_csv('/content/application_test.csv'),\n",
        "                pd.read_csv('/content/bureau.csv'),\n",
        "                pd.read_csv('/content/bureau_balance.csv'),\n",
        "                pd.read_csv('/content/POS_CASH_balance.csv'),\n",
        "                pd.read_csv('/content/credit_card_balance.csv'),\n",
        "                # pd.read_csv('/content/previous_application.csv'),\n",
        "                # pd.read_csv('/content/installments_payments.csv'),\n",
        "\n",
        "                # default-of-credit-card-clients-datasets\n",
        "                # pd.read_csv('/content/default_ucr.csv'),\n",
        "    ],\n",
        "\n",
        "    'name_dropped_columns':\n",
        "                [\n",
        "                 # home-credit-default-risk tables\n",
        "                 ['SK_ID_CURR', 'TARGET'],\n",
        "                 ['SK_ID_CURR'],\n",
        "                 ['SK_ID_CURR',\t'SK_ID_BUREAU'],\n",
        "                 ['SK_ID_BUREAU'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "\n",
        "                 # default-of-credit-card-clients-datasets\n",
        "                 ['ID', 'default.payment.next.month']              \n",
        "    ],   \n",
        "}\n",
        "\n",
        "# Keep ID and target columns separately\n",
        "dict['dropped_columns'] = [dict['datasets'][i][dict['name_dropped_columns'][i]] for i in range(len(dict['datasets']))]\n",
        "\n",
        "# Drop ID and target columns from the tables\n",
        "dict['datasets'] = [dict['datasets'][i].drop(dict['name_dropped_columns'][i], axis=1) for i in range(len(dict['datasets']))]\n",
        "\n",
        "[np.unique([str(dict['datasets'][i][column].dtype) for column in dict['datasets'][i].columns]) for i in range(len(dict['datasets']))]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['int64', 'object'], dtype='<U6'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lEyN2ye85kr",
        "outputId": "b7c80e9f-4e3c-409c-edf4-ae6e7fb98a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "X = dict['datasets'][0]\n",
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 1,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "s_time = time.time()\n",
        "fcleaning.emptyness_elimination(X)\n",
        "fcleaning.outliers_elimination(X)\n",
        "print('\\n n_jobs = 1, time:', time.time() - s_time)\n",
        "\n",
        "print('\\n CPUs avail:{}'.format(mp.cpu_count()))\n",
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 2,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "s_time = time.time()\n",
        "fcleaning.emptyness_elimination(X)\n",
        "fcleaning.outliers_elimination(X)\n",
        "print('\\n n_jobs = 2, time:', time.time() - s_time)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " n_jobs = 1, time: 6.621775150299072\n",
            "\n",
            " CPUs avail:2 \n",
            "\n",
            "\n",
            " n_jobs = 2, time: 6.631769895553589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L71oKQlpKba7",
        "outputId": "34219e3b-0900-4d1f-ad08-1b5b7d6cd6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X = dict['datasets'][2]\n",
        "fencoding = FEncoding(n_jobs = 1, \n",
        "                      chunks = None, \n",
        "                      path = None,)\n",
        "s_time = time.time()\n",
        "fencoding.dtime_to_data(X, \n",
        "                        dtime_col_names = {'ddays' : ['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE'],\n",
        "                                           'dmonths' : [],\n",
        "                                           'dyears' : []\n",
        "                                           },               \n",
        "                        time_encode = True, \n",
        "                        drop_current = True\n",
        "                     )\n",
        "f_dict = fencoding.pick_categor(X)\n",
        "print('\\n f_dict:', f_dict)\n",
        "fencoding.bucket_numerical(X, \n",
        "                         n_bins=5, \n",
        "                         columns_to_buck = 'all_numerical', \n",
        "                         drop_current = True)\n",
        "\n",
        "fencoding.encode_categor(X, method = 'OrdinalEncoder')\n",
        "print('\\n n_jobs = 1, time:', time.time() - s_time)\n",
        "\n",
        "fencoding = FEncoding(n_jobs = 2, \n",
        "                      chunks = None, \n",
        "                      path = None,)\n",
        "s_time = time.time()\n",
        "fencoding.dtime_to_data(X, \n",
        "                        dtime_col_names = {\n",
        "                                              'ddays' : ['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE'],\n",
        "                                              'dmonths' : [],\n",
        "                                              'dyears' : []\n",
        "                                           },               \n",
        "                        time_encode = True, \n",
        "                        drop_current = True\n",
        "                     )\n",
        "f_dict = fencoding.pick_categor(X)\n",
        "print('\\n f_dict:', f_dict)\n",
        "fencoding.bucket_numerical(X, \n",
        "                         n_bins=5, \n",
        "                         columns_to_buck = 'all_numerical', \n",
        "                         drop_current = True)\n",
        "\n",
        "fencoding.encode_categor(X, method = 'OrdinalEncoder')\n",
        "print('\\n n_jobs = 1, time:', time.time() - s_time)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " DAYS_CREDIT processed ...\n",
            "\n",
            " DAYS_CREDIT_ENDDATE processed ...\n",
            "\n",
            " DAYS_ENDDATE_FACT processed ...\n",
            "\n",
            " DAYS_CREDIT_UPDATE processed ...\n",
            "\n",
            " DAYS_CREDIT_date was encoded\n",
            "\n",
            " DAYS_CREDIT_ENDDATE_date was encoded\n",
            "\n",
            " DAYS_ENDDATE_FACT_date was encoded\n",
            "\n",
            " DAYS_CREDIT_UPDATE_date was encoded\n",
            "\n",
            " f_dict: {'categor_columns': ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'CNT_CREDIT_PROLONG', 'CREDIT_TYPE', 'DAYS_CREDIT_UPDATE'], 'numer_columns': ['DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_ANNUITY'], 'time_columns': []}\n",
            "\n",
            " DAYS_CREDIT_ENDDATE bucketing ...\n",
            "\n",
            " DAYS_ENDDATE_FACT bucketing ...\n",
            "\n",
            " AMT_CREDIT_MAX_OVERDUE bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_DEBT bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_LIMIT bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_OVERDUE bucketing ...\n",
            "\n",
            " AMT_ANNUITY bucketing ...\n",
            "\n",
            " n_jobs = 1, time: 18.792246341705322\n",
            "\n",
            " DAYS_CREDIT processed ...\n",
            "\n",
            " DAYS_CREDIT_UPDATE processed ...\n",
            "\n",
            " DAYS_CREDIT_ENDDATE processed ...\n",
            "\n",
            " DAYS_ENDDATE_FACT processed ...\n",
            "\n",
            " DAYS_CREDIT_UPDATE_date was encoded\n",
            "\n",
            " DAYS_CREDIT_date was encoded\n",
            "\n",
            " DAYS_CREDIT_ENDDATE_date was encoded\n",
            "\n",
            " DAYS_ENDDATE_FACT_date was encoded\n",
            "\n",
            " f_dict: {'categor_columns': ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'CNT_CREDIT_PROLONG', 'CREDIT_TYPE', 'DAYS_CREDIT_UPDATE'], 'numer_columns': ['DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_ANNUITY'], 'time_columns': []}\n",
            "\n",
            " AMT_CREDIT_SUM bucketing ...\n",
            "\n",
            " DAYS_CREDIT_ENDDATE bucketing ...\n",
            "\n",
            " DAYS_ENDDATE_FACT bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_DEBT bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_LIMIT bucketing ...\n",
            "\n",
            " AMT_CREDIT_MAX_OVERDUE bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_OVERDUE bucketing ...\n",
            "\n",
            " AMT_ANNUITY bucketing ...\n",
            "\n",
            " n_jobs = 1, time: 13.360267877578735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6FGGFT2H1eJ",
        "outputId": "031a9411-333c-4a4d-a1b8-7f3242991842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "X = dict['datasets'][2]\n",
        "X = fencoding.encode_categor(X, method = 'OrdinalEncoder')\n",
        "\n",
        "fimputation = FImputation('tree-based', \n",
        "            fill_with_value = 'extreme_values', \n",
        "                    n_jobs = 1,\n",
        ")\n",
        "s_time = time.time()\n",
        "fimputation.impute(X)\n",
        "print('\\n n_jobs = 1, time:', time.time() - s_time)\n",
        "\n",
        "fimputation = FImputation('tree-based', \n",
        "            fill_with_value = 'extreme_values', \n",
        "                    n_jobs = 2,\n",
        ")\n",
        "s_time = time.time()\n",
        "fimputation.impute(X)\n",
        "print('\\n n_jobs = 2, time:', time.time() - s_time)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " n_jobs = 1, time: 1.8499367237091064\n",
            "\n",
            " n_jobs = 2, time: 1.375610113143921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhMZDKFC28lW",
        "outputId": "f2a9b290-afa9-4842-c23b-63828e485162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 2,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "\n",
        "for i in range(len(dict['datasets'])):\n",
        "    X = dict['datasets'][i]\n",
        "    print('\\n dataset N {} | size: {} | emptyness_elimination ...'.format(i, X.shape))\n",
        "    s_time = time.time()\n",
        "    fcleaning.emptyness_elimination(X)   \n",
        "    print('\\n dataset N {}, emptyness_elimination completed successfully, time: {}'.format(i, time.time() - s_time))\n",
        "\n",
        "    s_time = time.time()\n",
        "    print('\\n dataset N {}  | size: {} | outliers_elimination ...'.format(i, X.shape))\n",
        "    fcleaning.outliers_elimination(X)\n",
        "    print('\\n dataset N {}, outliers_elimination completed successfully, time: {}'.format(i, time.time() - s_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " dataset N 0 | size: (307511, 120) | emptyness_elimination ...\n",
            "\n",
            " dataset N 0, emptyness_elimination completed successfully, time: 2.7245032787323\n",
            "\n",
            " dataset N 0  | size: (307511, 120) | outliers_elimination ...\n",
            "\n",
            " dataset N 0, outliers_elimination completed successfully, time: 4.1842169761657715\n",
            "\n",
            " dataset N 1 | size: (48744, 120) | emptyness_elimination ...\n",
            "\n",
            " FLAG_DOCUMENT_2, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_10, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_12, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_13, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_14, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_15, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_16, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_17, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_19, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_20, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_21, unique values: [0]\n",
            "\n",
            " dataset N 1, emptyness_elimination completed successfully, time: 0.6694204807281494\n",
            "\n",
            " dataset N 1  | size: (48744, 120) | outliers_elimination ...\n",
            "\n",
            " dataset N 1, outliers_elimination completed successfully, time: 0.9467957019805908\n",
            "\n",
            " dataset N 2 | size: (1716428, 15) | emptyness_elimination ...\n",
            "\n",
            " dataset N 2, emptyness_elimination completed successfully, time: 2.671640634536743\n",
            "\n",
            " dataset N 2  | size: (1716428, 15) | outliers_elimination ...\n",
            "\n",
            " dataset N 2, outliers_elimination completed successfully, time: 3.4559085369110107\n",
            "\n",
            " dataset N 3 | size: (27299925, 2) | emptyness_elimination ...\n",
            "\n",
            " dataset N 3, emptyness_elimination completed successfully, time: 6.400071144104004\n",
            "\n",
            " dataset N 3  | size: (27299925, 2) | outliers_elimination ...\n",
            "\n",
            " dataset N 3, outliers_elimination completed successfully, time: 7.082038164138794\n",
            "\n",
            " dataset N 4 | size: (10001358, 6) | emptyness_elimination ...\n",
            "\n",
            " dataset N 4, emptyness_elimination completed successfully, time: 7.526120901107788\n",
            "\n",
            " dataset N 4  | size: (10001358, 6) | outliers_elimination ...\n",
            "\n",
            " dataset N 4, outliers_elimination completed successfully, time: 9.860339164733887\n",
            "\n",
            " dataset N 5 | size: (3840312, 21) | emptyness_elimination ...\n",
            "\n",
            " dataset N 5, emptyness_elimination completed successfully, time: 8.498157262802124\n",
            "\n",
            " dataset N 5  | size: (3840312, 21) | outliers_elimination ...\n",
            "\n",
            " dataset N 5, outliers_elimination completed successfully, time: 10.86488676071167\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}