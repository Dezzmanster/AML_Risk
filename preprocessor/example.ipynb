{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yILP-zCWp_bG",
        "outputId": "de4709f5-0250-42a9-9150-e2d67a69c57c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "import os, copy, time\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# We will use multiprocessing for simple additive processes. Further, we will expend for other cases.\n",
        "# Sources: \n",
        "#https://medium.com/@vasista/parallel-processing-with-pandas-c76f88963005\n",
        "#https://towardsdatascience.com/speeding-up-and-perfecting-your-work-using-parallel-computing-8bc2f0c073f8\n",
        "import multiprocessing as mp\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# home-credit-default-risk tables\n",
        "if not Path(\"/content/application_test.csv\").is_file():\n",
        "  ! wget https://www.dropbox.com/s/j9xwcj9ixki5t2l/home-credit-default-risk.zip?dl=0 -O data.zip\n",
        "  ! unzip -q data.zip\n",
        "\n",
        "# default-of-credit-card-clients-dataset\n",
        "if not Path(\"/content/default_ucr.csv\").is_file():\n",
        "  ! wget https://www.dropbox.com/s/lj0d7qez18ea7dx/UCI_Credit_Card.csv?dl=0 -O default_ucr.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-20 13:05:58--  https://www.dropbox.com/s/j9xwcj9ixki5t2l/home-credit-default-risk.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/j9xwcj9ixki5t2l/home-credit-default-risk.zip [following]\n",
            "--2020-10-20 13:05:59--  https://www.dropbox.com/s/raw/j9xwcj9ixki5t2l/home-credit-default-risk.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc08b27bfe8d327997273accbbea.dl.dropboxusercontent.com/cd/0/inline/BBmdhBE6pmiU1YkX7FizJzG3RAxVxEIMM9mE3mZoqIftrSP-u0nSjKssMUw--ftiyggcPaW5o0fLY7kaC4YoU31VN12xYKonax1qlL4nDqWrXgaMz6siIgtLTD4uDgJmDbo/file# [following]\n",
            "--2020-10-20 13:05:59--  https://uc08b27bfe8d327997273accbbea.dl.dropboxusercontent.com/cd/0/inline/BBmdhBE6pmiU1YkX7FizJzG3RAxVxEIMM9mE3mZoqIftrSP-u0nSjKssMUw--ftiyggcPaW5o0fLY7kaC4YoU31VN12xYKonax1qlL4nDqWrXgaMz6siIgtLTD4uDgJmDbo/file\n",
            "Resolving uc08b27bfe8d327997273accbbea.dl.dropboxusercontent.com (uc08b27bfe8d327997273accbbea.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to uc08b27bfe8d327997273accbbea.dl.dropboxusercontent.com (uc08b27bfe8d327997273accbbea.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BBkEb7vuQyV6pEyosDnlMxgnoBYOXKvgQOGvxLggGu7eeo224MGSOLnS3L-CX9pQ8XIScAZ8c3HRkYxPA0qnPp6Z8iTqSXVqZJdrKIrvZoifQYIT9njGHRipjaPQPX5ir9n3IkX3cMZLAgz9vDPWzTeHyyPDFNMW0ywCZU_FfxVra-sx9pQgocc47vdkMDs_swGeNEDu1KEZlqjj6oguZxnmS54Hhom5w5oONgeMbSs5G2Pui9hogODVbgdST1vFwoTg3pa8McM2x7UZ5Z8USjuAjDcKT_ZrNoN7NEPUtKSXgN-4JqjoqhTlYTSOx3gjFNaqfwdDtt3eilLs1h8kdhNp40PYDqW2Q-4QhO16eoQxvw/file [following]\n",
            "--2020-10-20 13:06:00--  https://uc08b27bfe8d327997273accbbea.dl.dropboxusercontent.com/cd/0/inline2/BBkEb7vuQyV6pEyosDnlMxgnoBYOXKvgQOGvxLggGu7eeo224MGSOLnS3L-CX9pQ8XIScAZ8c3HRkYxPA0qnPp6Z8iTqSXVqZJdrKIrvZoifQYIT9njGHRipjaPQPX5ir9n3IkX3cMZLAgz9vDPWzTeHyyPDFNMW0ywCZU_FfxVra-sx9pQgocc47vdkMDs_swGeNEDu1KEZlqjj6oguZxnmS54Hhom5w5oONgeMbSs5G2Pui9hogODVbgdST1vFwoTg3pa8McM2x7UZ5Z8USjuAjDcKT_ZrNoN7NEPUtKSXgN-4JqjoqhTlYTSOx3gjFNaqfwdDtt3eilLs1h8kdhNp40PYDqW2Q-4QhO16eoQxvw/file\n",
            "Reusing existing connection to uc08b27bfe8d327997273accbbea.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 721616255 (688M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>] 688.19M  28.7MB/s    in 26s     \n",
            "\n",
            "2020-10-20 13:06:27 (26.0 MB/s) - ‘data.zip’ saved [721616255/721616255]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJjU_FZ7HrQX"
      },
      "source": [
        "from preprocessing import (FCleaning, \n",
        "                           FEncoding,\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHyQyU1XyavZ"
      },
      "source": [
        "# Read in the source datasets\n",
        "dict = {\n",
        "    'datasets':[\n",
        "                # home-credit-default-risk tables\n",
        "                pd.read_csv('/content/application_train.csv'),\n",
        "                pd.read_csv('/content/application_test.csv'),\n",
        "                pd.read_csv('/content/bureau.csv'),\n",
        "                pd.read_csv('/content/bureau_balance.csv'),\n",
        "                pd.read_csv('/content/POS_CASH_balance.csv'),\n",
        "                pd.read_csv('/content/credit_card_balance.csv'),\n",
        "                # pd.read_csv('/content/previous_application.csv'),\n",
        "                # pd.read_csv('/content/installments_payments.csv'),\n",
        "\n",
        "                # default-of-credit-card-clients-datasets\n",
        "                # pd.read_csv('/content/default_ucr.csv'),\n",
        "    ],\n",
        "\n",
        "    'name_dropped_columns':\n",
        "                [\n",
        "                 # home-credit-default-risk tables\n",
        "                 ['SK_ID_CURR', 'TARGET'],\n",
        "                 ['SK_ID_CURR'],\n",
        "                 ['SK_ID_CURR',\t'SK_ID_BUREAU'],\n",
        "                 ['SK_ID_BUREAU'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "\n",
        "                 # default-of-credit-card-clients-datasets\n",
        "                 ['ID', 'default.payment.next.month']              \n",
        "    ],   \n",
        "}\n",
        "\n",
        "# Keep ID and target columns separately\n",
        "dict['dropped_columns'] = [dict['datasets'][i][dict['name_dropped_columns'][i]] for i in range(len(dict['datasets']))]\n",
        "\n",
        "# Drop ID and target columns from the tables\n",
        "dict['datasets'] = [dict['datasets'][i].drop(dict['name_dropped_columns'][i], axis=1) for i in range(len(dict['datasets']))]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7j2FtHo2WhL",
        "outputId": "7f18c283-44da-4e25-abe4-a04e8a2b4655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "[np.unique([str(dict['datasets'][i][column].dtype) for column in dict['datasets'][i].columns]) for i in range(len(dict['datasets']))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['int64', 'object'], dtype='<U6'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiv-PXVq3EbC"
      },
      "source": [
        "X = dict['datasets'][0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgfKx-Hg-AXb",
        "outputId": "19caf9d6-1d8e-4abb-efcb-c484f307a2fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 1,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "\n",
        "s_time = time.time()\n",
        "fcleaning.emptyness_elimination(X)\n",
        "fcleaning.outliers_elimination(X)\n",
        "time.time() - s_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.082415342330933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lEyN2ye85kr",
        "outputId": "4e8b0aa1-f7f2-4696-db98-62c04f573176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print('\\n CPUs avail:{} \\n'.format(mp.cpu_count()))\n",
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 2,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "\n",
        "s_time = time.time()\n",
        "fcleaning.emptyness_elimination(X)\n",
        "fcleaning.outliers_elimination(X)\n",
        "time.time() - s_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " CPUs avail:2 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.717444658279419"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhMZDKFC28lW",
        "outputId": "f093445b-80f6-4d60-aed4-034080f88aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 2,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "\n",
        "for i in range(len(dict['datasets'])):\n",
        "    X = dict['datasets'][i]\n",
        "    print('\\n dataset N {} | size: {} | emptyness_elimination ...'.format(i, X.shape))\n",
        "    s_time = time.time()\n",
        "    fcleaning.emptyness_elimination(X)   \n",
        "    print('\\n dataset N {}, emptyness_elimination completed successfully, time: {}'.format(i, time.time() - s_time))\n",
        "\n",
        "    s_time = time.time()\n",
        "    print('\\n dataset N {}  | size: {} | outliers_elimination ...'.format(i, X.shape))\n",
        "    fcleaning.outliers_elimination(X)\n",
        "    print('\\n dataset N {}, outliers_elimination completed successfully, time: {}'.format(i, time.time() - s_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " dataset N 0 | size: (307511, 120) | emptyness_elimination ...\n",
            "\n",
            " dataset N 0, emptyness_elimination completed successfully, time: 4.282975196838379\n",
            "\n",
            " dataset N 0  | size: (307511, 120) | outliers_elimination ...\n",
            "\n",
            " dataset N 0, outliers_elimination completed successfully, time: 6.613495349884033\n",
            "\n",
            " dataset N 1 | size: (48744, 120) | emptyness_elimination ...\n",
            "\n",
            " FLAG_DOCUMENT_2, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_10, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_12, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_13, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_14, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_15, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_16, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_17, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_19, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_20, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_21, unique values: [0]\n",
            "\n",
            " dataset N 1, emptyness_elimination completed successfully, time: 1.0747668743133545\n",
            "\n",
            " dataset N 1  | size: (48744, 120) | outliers_elimination ...\n",
            "\n",
            " dataset N 1, outliers_elimination completed successfully, time: 1.6841328144073486\n",
            "\n",
            " dataset N 2 | size: (1716428, 15) | emptyness_elimination ...\n",
            "\n",
            " dataset N 2, emptyness_elimination completed successfully, time: 4.364962339401245\n",
            "\n",
            " dataset N 2  | size: (1716428, 15) | outliers_elimination ...\n",
            "\n",
            " dataset N 2, outliers_elimination completed successfully, time: 5.494971752166748\n",
            "\n",
            " dataset N 3 | size: (27299925, 2) | emptyness_elimination ...\n",
            "\n",
            " dataset N 3, emptyness_elimination completed successfully, time: 8.956387281417847\n",
            "\n",
            " dataset N 3  | size: (27299925, 2) | outliers_elimination ...\n",
            "\n",
            " dataset N 3, outliers_elimination completed successfully, time: 10.945451021194458\n",
            "\n",
            " dataset N 4 | size: (10001358, 6) | emptyness_elimination ...\n",
            "\n",
            " dataset N 4, emptyness_elimination completed successfully, time: 10.167070388793945\n",
            "\n",
            " dataset N 4  | size: (10001358, 6) | outliers_elimination ...\n",
            "\n",
            " dataset N 4, outliers_elimination completed successfully, time: 15.956858158111572\n",
            "\n",
            " dataset N 5 | size: (3840312, 21) | emptyness_elimination ...\n",
            "\n",
            " dataset N 5, emptyness_elimination completed successfully, time: 11.939577579498291\n",
            "\n",
            " dataset N 5  | size: (3840312, 21) | outliers_elimination ...\n",
            "\n",
            " dataset N 5, outliers_elimination completed successfully, time: 17.283265829086304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6FGGFT2H1eJ"
      },
      "source": [
        "X = dict['datasets'][2]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-IIdbfRIGEi"
      },
      "source": [
        "fencoding = FEncoding(n_jobs = 2, \n",
        "                      chunks = None, \n",
        "                      path = None,)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8i0aoTiH1kC",
        "outputId": "077bb766-0c47-4aa6-bbef-963cd6d47881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "source": [
        "s_time = time.time()\n",
        "fencoding.encode_categor(X, method = 'OrdinalEncoder')\n",
        "fencoding.bucket_numerical(X, \n",
        "                         n_bins=5, \n",
        "                         columns_to_buck = 'all_numerical', \n",
        "                         drop_current = True)\n",
        "f_dict = fencoding.pick_categor(X)\n",
        "print('\\n f_dict:', f_dict)\n",
        "fencoding.dtime_to_data(X, \n",
        "                        dtime_col_names = {\n",
        "                                              'ddays' : ['DAYS_CREDIT', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'DAYS_CREDIT_UPDATE'],\n",
        "                                              'dmonths' : [],\n",
        "                                              'dyears' : []\n",
        "                                           },               \n",
        "                        time_encode = True, \n",
        "                        drop_current = True\n",
        "                     )\n",
        "print()  \n",
        "time.time() - s_time"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " DAYS_CREDIT_ENDDATE bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM bucketing ...\n",
            "\n",
            " DAYS_ENDDATE_FACT bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_DEBT bucketing ...\n",
            "\n",
            " AMT_CREDIT_MAX_OVERDUE bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_LIMIT bucketing ...\n",
            "\n",
            " AMT_CREDIT_SUM_OVERDUE bucketing ...\n",
            "\n",
            " AMT_ANNUITY bucketing ...\n",
            "\n",
            " f_dict: {'categor_columns': ['CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'CNT_CREDIT_PROLONG', 'CREDIT_TYPE', 'DAYS_CREDIT_UPDATE'], 'numer_columns': ['DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_ANNUITY'], 'time_columns': []}\n",
            "\n",
            " DAYS_CREDIT processed ...\n",
            " DAYS_CREDIT_UPDATE processed ...\n",
            "\n",
            "\n",
            " DAYS_CREDIT_ENDDATE processed ...\n",
            "\n",
            " DAYS_ENDDATE_FACT processed ...\n",
            "\n",
            " DAYS_CREDIT_UPDATE_date was encoded\n",
            "\n",
            " DAYS_CREDIT_date was encoded\n",
            "\n",
            " DAYS_CREDIT_ENDDATE_date was encoded\n",
            "\n",
            " DAYS_ENDDATE_FACT_date was encoded\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.693140983581543"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys5_F0KZH1nB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0k_4uy6H1g7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djhAiqWC9bNQ"
      },
      "source": [
        "class FImputation(object):\n",
        "    '''\n",
        "      Dealing with missing values:\n",
        "      We will use simple techniques with regards to the model that we use.\n",
        "      For tree-based models, nana will be filled in with max values (or zeros)\n",
        "      For regression with means and medians for numerical and categorical types respectively.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, model_type, fill_with_value = None, \n",
        "                    n_jobs = 1, chunks = None, \n",
        "                    path = None,\n",
        "                    ):\n",
        "        \n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.fill_with_value = fill_with_value\n",
        "        \n",
        "        self.n_jobs = n_jobs\n",
        "        self.chunks = chunks\n",
        "        self.path = path\n",
        "\n",
        "\n",
        "    def impute_(self, X):\n",
        "        if self.model_type == 'tree-based':                         \n",
        "            imputer = SimpleImputer(missing_values=[np.nan], # what else?\n",
        "                                    strategy='constant'\n",
        "                                    )\n",
        "            if self.fill_with_value == 'zeros':\n",
        "                #imputer.set_params(fill_value = 0)\n",
        "                #return imputer.fit_transform(X)\n",
        "                X.fillna(0, inplace=True)                        \n",
        "                return X\n",
        "\n",
        "            elif self.fill_with_value == 'extreme_values':\n",
        "                for column in X.columns:\n",
        "                    #imputer.set_params(fill_value = X['AMT_INCOME_TOTAL'][abs(X['AMT_INCOME_TOTAL']) == abs(X['AMT_INCOME_TOTAL']).max()].item())\n",
        "                    #X[column] = imputer.fit_transform(np.array(X[column].values).reshape(-1,1))\n",
        "                    X[column].fillna( X['AMT_INCOME_TOTAL'][abs(X['AMT_INCOME_TOTAL']) == abs(X['AMT_INCOME_TOTAL']).max()].item(), inplace=True)\n",
        "                return X\n",
        "            else:\n",
        "                raise VlaueError('Identify fill_with_value parameter')\n",
        "\n",
        "        if self.model_type == 'regression-based':\n",
        "            #TODO\n",
        "            strategies = ['mean', 'median']\n",
        "            col_types = []\n",
        "\n",
        "    def impute(self, X):\n",
        "        if self.chunks == None:\n",
        "            self.chunks  = int(len(X.columns)/self.n_jobs)\n",
        "            p = Pool(processes = self.n_jobs)\n",
        "            X =  pd.concat(p.map(self.impute_, \n",
        "                                    [X[list(X.columns)[start: start + self.chunks]] for start in range(0, len(X.columns), self.chunks)]\n",
        "                                    ), axis=1)\n",
        "            if self.path != None:\n",
        "                  X.to_csv(self.path)\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Guv6qbCUz4r",
        "outputId": "15b0b3d9-f36e-4996-b664-b91d9ae0941b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fimputation = FImputation('tree-based',\n",
        "                          fill_with_value='zeros',\n",
        "                          n_jobs=1\n",
        "                         )\n",
        "s_time = time.time()\n",
        "fimputation.impute(X)\n",
        "time.time() - s_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.768680095672607"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM1vnnTp-oxs",
        "outputId": "228f4717-5106-4c8c-d76b-e2a513fa2640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fimputation = FImputation('tree-based',\n",
        "                          fill_with_value='zeros',\n",
        "                          n_jobs=2\n",
        "                         )\n",
        "s_time = time.time()\n",
        "fimputation.impute(X)\n",
        "time.time() - s_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.519124746322632"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqml3lx3Un3h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R76KT0ZebVWN"
      },
      "source": [
        "class FScaling(object):\n",
        "  def __init__(self, scaler):\n",
        "    self.scaler = scaler\n",
        "\n",
        "  def scale(X):\n",
        "\n",
        "    return X\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}