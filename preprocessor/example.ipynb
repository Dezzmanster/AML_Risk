{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yILP-zCWp_bG"
      },
      "source": [
        "import os, copy, time\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# We will use multiprocessing for simple additive processes. Further, we will expend for other cases.\n",
        "# Sources: \n",
        "#https://medium.com/@vasista/parallel-processing-with-pandas-c76f88963005\n",
        "#https://towardsdatascience.com/speeding-up-and-perfecting-your-work-using-parallel-computing-8bc2f0c073f8\n",
        "import multiprocessing as mp\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# home-credit-default-risk tables\n",
        "if not Path(\"/content/application_test.csv\").is_file():\n",
        "  ! wget https://www.dropbox.com/s/j9xwcj9ixki5t2l/home-credit-default-risk.zip?dl=0 -O data.zip\n",
        "  ! unzip -q data.zip\n",
        "\n",
        "# default-of-credit-card-clients-dataset\n",
        "if not Path(\"/content/default_ucr.csv\").is_file():\n",
        "  ! wget https://www.dropbox.com/s/lj0d7qez18ea7dx/UCI_Credit_Card.csv?dl=0 -O default_ucr.csv\n",
        "\n",
        "from preprocessing import (FCleaning,   \n",
        "                          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHyQyU1XyavZ"
      },
      "source": [
        "# Read in the source datasets\n",
        "dict = {\n",
        "    'datasets':[\n",
        "                # home-credit-default-risk tables\n",
        "                pd.read_csv('/content/application_train.csv'),\n",
        "                pd.read_csv('/content/application_test.csv'),\n",
        "                pd.read_csv('/content/bureau.csv'),\n",
        "                pd.read_csv('/content/bureau_balance.csv'),\n",
        "                pd.read_csv('/content/POS_CASH_balance.csv'),\n",
        "                pd.read_csv('/content/credit_card_balance.csv'),\n",
        "                # pd.read_csv('/content/previous_application.csv'),\n",
        "                # pd.read_csv('/content/installments_payments.csv'),\n",
        "\n",
        "                # default-of-credit-card-clients-datasets\n",
        "                # pd.read_csv('/content/default_ucr.csv'),\n",
        "    ],\n",
        "\n",
        "    'name_dropped_columns':\n",
        "                [\n",
        "                 # home-credit-default-risk tables\n",
        "                 ['SK_ID_CURR', 'TARGET'],\n",
        "                 ['SK_ID_CURR'],\n",
        "                 ['SK_ID_CURR',\t'SK_ID_BUREAU'],\n",
        "                 ['SK_ID_BUREAU'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "                 ['SK_ID_PREV',\t'SK_ID_CURR'],\n",
        "\n",
        "                 # default-of-credit-card-clients-datasets\n",
        "                 ['ID', 'default.payment.next.month']              \n",
        "    ],   \n",
        "}\n",
        "\n",
        "# Keep ID and target columns separately\n",
        "dict['dropped_columns'] = [dict['datasets'][i][dict['name_dropped_columns'][i]] for i in range(len(dict['datasets']))]\n",
        "\n",
        "# Drop ID and target columns from the tables\n",
        "dict['datasets'] = [dict['datasets'][i].drop(dict['name_dropped_columns'][i], axis=1) for i in range(len(dict['datasets']))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7j2FtHo2WhL",
        "outputId": "7f18c283-44da-4e25-abe4-a04e8a2b4655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "[np.unique([str(dict['datasets'][i][column].dtype) for column in dict['datasets'][i].columns]) for i in range(len(dict['datasets']))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['int64', 'object'], dtype='<U6'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7'),\n",
              " array(['float64', 'int64', 'object'], dtype='<U7')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiv-PXVq3EbC"
      },
      "source": [
        "X = dict['datasets'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgfKx-Hg-AXb",
        "outputId": "19caf9d6-1d8e-4abb-efcb-c484f307a2fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 1,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "\n",
        "s_time = time.time()\n",
        "fcleaning.emptyness_elimination(X)\n",
        "fcleaning.outliers_elimination(X)\n",
        "time.time() - s_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.082415342330933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lEyN2ye85kr",
        "outputId": "4e8b0aa1-f7f2-4696-db98-62c04f573176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print('\\n CPUs avail:{} \\n'.format(mp.cpu_count()))\n",
        "fcleaning = FCleaning('iqr_proximity_rule', \n",
        "                      n_jobs = 2,\n",
        "                      chunks = None,\n",
        "                      #path = './trial1.csv', #None,\n",
        "                      )\n",
        "\n",
        "s_time = time.time()\n",
        "fcleaning.emptyness_elimination(X)\n",
        "fcleaning.outliers_elimination(X)\n",
        "time.time() - s_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " CPUs avail:2 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.717444658279419"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhMZDKFC28lW",
        "outputId": "f093445b-80f6-4d60-aed4-034080f88aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(len(dict['datasets'])):\n",
        "    X = dict['datasets'][i]\n",
        "    print('\\n dataset N {} | size: {} | emptyness_elimination ...'.format(i, X.shape))\n",
        "    s_time = time.time()\n",
        "    fcleaning.emptyness_elimination(X)   \n",
        "    print('\\n dataset N {}, emptyness_elimination completed successfully, time: {}'.format(i, time.time() - s_time))\n",
        "\n",
        "    s_time = time.time()\n",
        "    print('\\n dataset N {}  | size: {} | outliers_elimination ...'.format(i, X.shape))\n",
        "    fcleaning.outliers_elimination(X)\n",
        "    print('\\n dataset N {}, outliers_elimination completed successfully, time: {}'.format(i, time.time() - s_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " dataset N 0 | size: (307511, 120) | emptyness_elimination ...\n",
            "\n",
            " dataset N 0, emptyness_elimination completed successfully, time: 4.282975196838379\n",
            "\n",
            " dataset N 0  | size: (307511, 120) | outliers_elimination ...\n",
            "\n",
            " dataset N 0, outliers_elimination completed successfully, time: 6.613495349884033\n",
            "\n",
            " dataset N 1 | size: (48744, 120) | emptyness_elimination ...\n",
            "\n",
            " FLAG_DOCUMENT_2, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_10, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_12, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_13, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_14, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_15, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_16, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_17, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_19, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_20, unique values: [0]\n",
            "\n",
            " FLAG_DOCUMENT_21, unique values: [0]\n",
            "\n",
            " dataset N 1, emptyness_elimination completed successfully, time: 1.0747668743133545\n",
            "\n",
            " dataset N 1  | size: (48744, 120) | outliers_elimination ...\n",
            "\n",
            " dataset N 1, outliers_elimination completed successfully, time: 1.6841328144073486\n",
            "\n",
            " dataset N 2 | size: (1716428, 15) | emptyness_elimination ...\n",
            "\n",
            " dataset N 2, emptyness_elimination completed successfully, time: 4.364962339401245\n",
            "\n",
            " dataset N 2  | size: (1716428, 15) | outliers_elimination ...\n",
            "\n",
            " dataset N 2, outliers_elimination completed successfully, time: 5.494971752166748\n",
            "\n",
            " dataset N 3 | size: (27299925, 2) | emptyness_elimination ...\n",
            "\n",
            " dataset N 3, emptyness_elimination completed successfully, time: 8.956387281417847\n",
            "\n",
            " dataset N 3  | size: (27299925, 2) | outliers_elimination ...\n",
            "\n",
            " dataset N 3, outliers_elimination completed successfully, time: 10.945451021194458\n",
            "\n",
            " dataset N 4 | size: (10001358, 6) | emptyness_elimination ...\n",
            "\n",
            " dataset N 4, emptyness_elimination completed successfully, time: 10.167070388793945\n",
            "\n",
            " dataset N 4  | size: (10001358, 6) | outliers_elimination ...\n",
            "\n",
            " dataset N 4, outliers_elimination completed successfully, time: 15.956858158111572\n",
            "\n",
            " dataset N 5 | size: (3840312, 21) | emptyness_elimination ...\n",
            "\n",
            " dataset N 5, emptyness_elimination completed successfully, time: 11.939577579498291\n",
            "\n",
            " dataset N 5  | size: (3840312, 21) | outliers_elimination ...\n",
            "\n",
            " dataset N 5, outliers_elimination completed successfully, time: 17.283265829086304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djhAiqWC9bNQ"
      },
      "source": [
        "class FImputation(object):\n",
        "    '''\n",
        "      Dealing with missing values:\n",
        "      We will use simple techniques with regards to the model that we use.\n",
        "      For tree-based models, nana will be filled in with max values (or zeros)\n",
        "      For regression with means and medians for numerical and categorical types respectively.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, model_type, fill_with_value = None, \n",
        "                    n_jobs = 1, chunks = None, \n",
        "                    path = None,\n",
        "                    ):\n",
        "        \n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.fill_with_value = fill_with_value\n",
        "        \n",
        "        self.n_jobs = n_jobs\n",
        "        self.chunks = chunks\n",
        "        self.path = path\n",
        "\n",
        "\n",
        "    def impute_(self, X):\n",
        "        if self.model_type == 'tree-based':                         \n",
        "            imputer = SimpleImputer(missing_values=[np.nan], # what else?\n",
        "                                    strategy='constant'\n",
        "                                    )\n",
        "            if self.fill_with_value == 'zeros':\n",
        "                #imputer.set_params(fill_value = 0)\n",
        "                #return imputer.fit_transform(X)\n",
        "                X.fillna(0, inplace=True)                        \n",
        "                return X\n",
        "\n",
        "            elif self.fill_with_value == 'extreme_values':\n",
        "                for column in X.columns:\n",
        "                    #imputer.set_params(fill_value = X['AMT_INCOME_TOTAL'][abs(X['AMT_INCOME_TOTAL']) == abs(X['AMT_INCOME_TOTAL']).max()].item())\n",
        "                    #X[column] = imputer.fit_transform(np.array(X[column].values).reshape(-1,1))\n",
        "                    X[column].fillna( X['AMT_INCOME_TOTAL'][abs(X['AMT_INCOME_TOTAL']) == abs(X['AMT_INCOME_TOTAL']).max()].item(), inplace=True)\n",
        "                return X\n",
        "            else:\n",
        "                raise VlaueError('Identify fill_with_value parameter')\n",
        "\n",
        "        if self.model_type == 'regression-based':\n",
        "            #TODO\n",
        "            strategies = ['mean', 'median']\n",
        "            col_types = []\n",
        "\n",
        "    def impute(self, X):\n",
        "        if self.chunks == None:\n",
        "            self.chunks  = int(len(X.columns)/self.n_jobs)\n",
        "            p = Pool(processes = self.n_jobs)\n",
        "            X =  pd.concat(p.map(self.impute_, \n",
        "                                    [X[list(X.columns)[start: start + self.chunks]] for start in range(0, len(X.columns), self.chunks)]\n",
        "                                    ), axis=1)\n",
        "            if self.path != None:\n",
        "                  X.to_csv(self.path)\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Guv6qbCUz4r",
        "outputId": "15b0b3d9-f36e-4996-b664-b91d9ae0941b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fimputation = FImputation('tree-based',\n",
        "                          fill_with_value='zeros',\n",
        "                          n_jobs=1\n",
        "                         )\n",
        "s_time = time.time()\n",
        "fimputation.impute(X)\n",
        "time.time() - s_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.768680095672607"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM1vnnTp-oxs",
        "outputId": "228f4717-5106-4c8c-d76b-e2a513fa2640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "fimputation = FImputation('tree-based',\n",
        "                          fill_with_value='zeros',\n",
        "                          n_jobs=2\n",
        "                         )\n",
        "s_time = time.time()\n",
        "fimputation.impute(X)\n",
        "time.time() - s_time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.519124746322632"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu-q3X5y4WHQ"
      },
      "source": [
        "class FEncoding(object):\n",
        "    def __init__(self, \n",
        "                 n_jobs = 1, chunks = None, \n",
        "                 path = None,\n",
        "                 ):\n",
        "      \n",
        "        self.n_jobs = n_jobs\n",
        "        self.chunks = chunks\n",
        "        self.path = path\n",
        "\n",
        "        self.categor_types = ['object', 'bool', 'int32', 'int64']\n",
        "        self.numer_types = ['float', 'float32', 'float64']\n",
        "\n",
        "        self.categor_columns = []\n",
        "        self.numer_columns = []\n",
        "    \n",
        "    def dtime_to_data_(self, X, \n",
        "                       dtime_col_names = {\n",
        "                           'ddays' : [],\n",
        "                           'dmonths' : [],\n",
        "                           'dyears' : []\n",
        "                           },\n",
        "                        \n",
        "                       start_date = '2020-01-01', \n",
        "                       time_encode = False, \n",
        "                       drop_current = False):\n",
        "      \n",
        "        # Time index\n",
        "        start_date = pd.Timestamp(start_date)\n",
        "\n",
        "        for delays, k in zip([list(dtime_col_names.keys()), [1, 30, 365]]): \n",
        "            for column in dtime_col_names['ddays']:\n",
        "                X[column + '_date'] = start_date + pd.to_timedelta(X[column]*k, 'D')\n",
        "                if drop_current:\n",
        "                    X.drop(columns=[column], inplace=True)\n",
        "\n",
        "        if time_encode:        \n",
        "            for column in X.columns:\n",
        "              if str(X[column].dtype) == 'datetime64[ns]': \n",
        "                  # TODO: check if there are any other time types\n",
        "                  # datetime64[ns] -> int64\n",
        "                  X[column + '_year'] = X[column].dt.year\n",
        "                  X[column + '_month'] = X[column].dt.month\n",
        "                  X[column + '_day'] = X[column].dt.day\n",
        "                  if drop_current:\n",
        "                        X.drop(columns=[column], inplace=True)\n",
        "        return X    \n",
        "    \n",
        "    def pick_categor_(self, X):\n",
        "        # Sometimes categorical feature can be presented with a float type. \n",
        "        # Let's check for that\n",
        "        n_unique = 50\n",
        "        len_X = len(X)\n",
        "        unique_values_X = X[column].unique()\n",
        "        for column in X.columns:\n",
        "            c_type = str(X[column].dtype) \n",
        "            \n",
        "            if any(c_type == t for t in self.numer_types) & (len(unique_values_X) < percent_unique):\n",
        "                print('\\n {} has type {} and unique values: {} -> {}, will be considered as categorical \\n'.format(column, c_type, unique_values_X))\n",
        "                self.categor_columns.append(column)\n",
        "\n",
        "            elif any(c_type == t for t in self.categor_types):\n",
        "                self.categor_columns.append(column)\n",
        "\n",
        "            else:\n",
        "                self.numer_columns.append(column)\n",
        "    \n",
        "    def bucket_numerical_(self, X, n_bins=5, drop_current = False):\n",
        "        # TODO: specify or introduce a criterion which columns to bake\n",
        "        # K-bins discretization\n",
        "        discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal') \n",
        "        for column in X.columns:\n",
        "            if any(column == col for col in self.numer_columns):\n",
        "                X[column + '_bucketed'] = discretizer.fit_transform(np.array(X[column].values).reshape(-1,1))\n",
        "            if drop_current:\n",
        "                X.drop(columns=[column], inplace=True)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def encode_categor_(self, X):\n",
        "\n",
        "      \n",
        "        return X\n",
        "\n",
        "\n",
        "    def dtime_to_data(self, X, dtime_col_names, start_day, time_encode = False):\n",
        "\n",
        "        return X\n",
        "\n",
        "    def pick_categor(self, X):\n",
        "        return X\n",
        "    \n",
        "    def bucket_numerical(self, X):\n",
        "        return X\n",
        "\n",
        "    def encode_categor(self, X):\n",
        "        return X\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOjJcqUTWS92"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nnlej87aH1A2",
        "outputId": "b3b354f4-7180-49ef-d6eb-8ca02f7edabd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "pd.qcut(X[['AMT_REQ_CREDIT_BUREAU_HOUR']].values.ravel(), 3, labels=[\"good\", \"medium\", \"bad\"], duplicates='drop')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-01843fb6d8df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AMT_REQ_CREDIT_BUREAU_HOUR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"good\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"medium\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bad\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36mqcut\u001b[0;34m(x, q, labels, retbins, precision, duplicates)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0minclude_lowest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mduplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mduplicates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36m_bins_to_cuts\u001b[0;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 435\u001b[0;31m                     \u001b[0;34m\"Bin labels must be one fewer than the number of bin edges\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m                 )\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Bin labels must be one fewer than the number of bin edges"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXKJTj-8GyFm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B5_dMG_Unzm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orRGLTzbUn7x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFmeHoH-Un5x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqml3lx3Un3h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R76KT0ZebVWN"
      },
      "source": [
        "class FScaling(object):\n",
        "  def __init__(self, scaler):\n",
        "    self.scaler = scaler\n",
        "\n",
        "  def scale(X):\n",
        "\n",
        "    return X\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}